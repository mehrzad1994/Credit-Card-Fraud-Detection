{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'catboostbucket'\n",
    "prefix = 'dataset'\n",
    "key = 'dataset/credit_card_transactions-ibm_v2.csv'\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import io\n",
    "s3 = boto3.client('s3')\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Card</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Use Chip</th>\n",
       "      <th>Merchant Name</th>\n",
       "      <th>Merchant City</th>\n",
       "      <th>Merchant State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Errors?</th>\n",
       "      <th>Is Fraud?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>06:21</td>\n",
       "      <td>$134.09</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>3527213246127876953</td>\n",
       "      <td>La Verne</td>\n",
       "      <td>CA</td>\n",
       "      <td>91750.0</td>\n",
       "      <td>5300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>06:42</td>\n",
       "      <td>$38.48</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-727612092139916043</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>06:22</td>\n",
       "      <td>$120.34</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-727612092139916043</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>17:45</td>\n",
       "      <td>$128.95</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>3414527459579106770</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>06:23</td>\n",
       "      <td>$104.71</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>5817218446178736267</td>\n",
       "      <td>La Verne</td>\n",
       "      <td>CA</td>\n",
       "      <td>91750.0</td>\n",
       "      <td>5912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User  Card  Year  Month  Day   Time   Amount           Use Chip  \\\n",
       "0     0     0  2002      9    1  06:21  $134.09  Swipe Transaction   \n",
       "1     0     0  2002      9    1  06:42   $38.48  Swipe Transaction   \n",
       "2     0     0  2002      9    2  06:22  $120.34  Swipe Transaction   \n",
       "3     0     0  2002      9    2  17:45  $128.95  Swipe Transaction   \n",
       "4     0     0  2002      9    3  06:23  $104.71  Swipe Transaction   \n",
       "\n",
       "         Merchant Name  Merchant City Merchant State      Zip   MCC Errors?  \\\n",
       "0  3527213246127876953       La Verne             CA  91750.0  5300     NaN   \n",
       "1  -727612092139916043  Monterey Park             CA  91754.0  5411     NaN   \n",
       "2  -727612092139916043  Monterey Park             CA  91754.0  5411     NaN   \n",
       "3  3414527459579106770  Monterey Park             CA  91754.0  5651     NaN   \n",
       "4  5817218446178736267       La Verne             CA  91750.0  5912     NaN   \n",
       "\n",
       "  Is Fraud?  \n",
       "0        No  \n",
       "1        No  \n",
       "2        No  \n",
       "3        No  \n",
       "4        No  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  accessing the data set in catboostbucket\n",
    "obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "df = pd.read_csv(obj.get(\"Body\"),nrows=1000000)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all preprocessing\n",
    "df['Is Fraud?']=df['Is Fraud?'].replace({'No':0,'Yes':1}) \n",
    "df['Errors?']=df['Errors?'].fillna('NAN')\n",
    "df['Errors?']=df['Errors?'].apply(lambda value:value=='NAN')\n",
    "df['Use Chip'].unique()\n",
    "df['is online']=df['Use Chip'].apply(lambda value:value=='Online Transaction')\n",
    "df['Use Chip']=df['Use Chip'].replace({'Swipe Transaction':0, 'Online Transaction':1, 'Chip Transaction':2})\n",
    "df['Zip'] = df['Zip'].fillna(df['Zip'].mean())  \n",
    "df['Amount'] = df['Amount'].apply(lambda value: float(value.split(\"$\")[1]))\n",
    "df['Hour'] = df['Time'].apply(lambda value: int(value.split(\":\")[0]))\n",
    "df['Minutes'] = df['Time'].apply(lambda value: int(value.split(\":\")[1]))\n",
    "df.drop(['Time'], axis=1, inplace=True)     \n",
    "df['Merchant State']=df['Merchant State'].fillna('NAN')    \n",
    "df['Merchant City']=df['Merchant City'].fillna('NAN') \n",
    "df['is vozmes']=df['Amount'].apply(lambda value: value<0)  \n",
    "df['abs_amount']=df['Amount'].apply(lambda value: abs(value))\n",
    "le=LabelEncoder() \n",
    "df['Merchant State']=le.fit_transform(df['Merchant State'])\n",
    "le=LabelEncoder()\n",
    "df['Merchant City']=le.fit_transform(df['Merchant City'])\n",
    "df.drop('Merchant Name',axis=1,inplace=True)\n",
    "df.drop('User',axis=1,inplace=True)\n",
    "df = pd.concat([df['Is Fraud?'], df.drop(['Is Fraud?'], axis=1)], axis=1)\n",
    "df.replace({False: 0, True: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker does not want the xtest ytest , instead first column should be the target column, so np.split is used instead\n",
    "train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=42), [int(0.7 * len(df)), int(0.9 * len(df))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)\n",
    "test_data.to_csv('test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making folders and ... in bucket for new files \n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')\n",
    "s3_input_train = TrainingInput(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = TrainingInput(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-18 22:37:11 Starting - Starting the training job...ProfilerReport-1650321431: InProgress\n",
      "............\n",
      "2022-04-18 22:39:37 Starting - Preparing the instances for training..............................\n",
      "2022-04-18 22:44:38 Downloading - Downloading input data...\n",
      "2022-04-18 22:45:08 Training - Downloading the training image..........\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2022-04-18:22:46:48:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2022-04-18:22:46:48:INFO] File size need to be processed in the node: 52.82mb. Available memory size in the node: 8467.79mb\u001b[0m\n",
      "\u001b[34m[2022-04-18:22:46:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[22:46:48] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[22:46:49] 700000x16 matrix with 11200000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2022-04-18:22:46:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[22:46:49] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[22:46:49] 200000x16 matrix with 3200000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[22:46:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.000983#011validation-error:0.00106\u001b[0m\n",
      "\u001b[34m[22:46:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.000983#011validation-error:0.00106\u001b[0m\n",
      "\u001b[34m[22:46:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.000983#011validation-error:0.00106\u001b[0m\n",
      "\u001b[34m[22:46:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.000819#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.000819#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.000819#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.000819#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.000816#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.000816#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.000816#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.00081#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.000811#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.000807#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.00081#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.000807#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.000809#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:46:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.000807#011validation-error:0.000885\u001b[0m\n",
      "\u001b[34m[22:47:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.000769#011validation-error:0.00086\u001b[0m\n",
      "\u001b[34m[22:47:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.000771#011validation-error:0.00086\u001b[0m\n",
      "\u001b[34m[22:47:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.00077#011validation-error:0.00086\u001b[0m\n",
      "\n",
      "2022-04-18 22:46:59 Training - Training image download completed. Training in progress.\u001b[34m[22:47:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.000767#011validation-error:0.00086\u001b[0m\n",
      "\u001b[34m[22:47:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.000767#011validation-error:0.00086\u001b[0m\n",
      "\u001b[34m[22:47:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.000767#011validation-error:0.00086\u001b[0m\n",
      "\u001b[34m[22:47:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.000586#011validation-error:0.0007\u001b[0m\n",
      "\u001b[34m[22:47:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.000581#011validation-error:0.00069\u001b[0m\n",
      "\n",
      "2022-04-18 22:48:39 Uploading - Uploading generated training modelProfilerReport-1650321431: NoIssuesFound\n",
      "\n",
      "2022-04-18 22:49:45 Completed - Training job completed\n",
      "Training seconds: 326\n",
      "Billable seconds: 326\n"
     ]
    }
   ],
   "source": [
    "# accessing builtin algorithm of xgboost within aws in containers and then giving estimator the model and other info to prep for training\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'}\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "xgb = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(eta=0.1, objective='binary:logistic', num_round=25) \n",
    "\n",
    "# training the model\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "# deploying the model, this is the last cell which is organized, below is messy, ignore commented out cells.\n",
    "xgb_predictor = xgb.deploy(\n",
    "\tinitial_instance_count = 1,\n",
    "\tinstance_type = 'ml.m4.xlarge',\n",
    "\tserializer = CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(data, rows=500):\n",
    "#     split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "#     predictions = ''\n",
    "#     for array in split_array:\n",
    "#         predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "        \n",
    "\n",
    "#     return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "# predictions = predict(test_data.to_numpy()[:,1:])\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test to see if we did everything correctly\n",
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "        p = np.fromstring(predictions[1:], sep=',')\n",
    "        p = np.round(p)\n",
    "\n",
    "    return p\n",
    "\n",
    "predictions = predict(test_data.to_numpy()[:,1:])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.argmax(predictions)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.round(predictions)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = (x == 1).sum()\n",
    "# print('Total occurences of \"1\" in array: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = (x == 0).sum()\n",
    "# print('Total occurences of \"1\" in array: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletes endpoint\n",
    "# xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.040032703429460526',\n",
       " '0.040032703429460526',\n",
       " '0.040032703429460526',\n",
       " '0.040032703429460526']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this was the first version of lambda function I wrote, its better now. but basically endpoint_name is the name of the deployed model and then once we give it input_json it retuns the predictions\n",
    "import boto3\n",
    "import numpy as np\n",
    "# copy endpoint name u got from endpoint name print() or uploaded one and copy it here\n",
    "ENDPOINT_NAME = 'xgboost-2022-04-18-22-50-13-637'\n",
    "runtime = boto3.client('runtime.sagemaker')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "  inputs = event['data']\n",
    "  result = []\n",
    "  for input in inputs:\n",
    "    serialized_input = ','.join(map(str, input))\n",
    "\n",
    "    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                     ContentType='text/csv',\n",
    "                                     Body=serialized_input)\n",
    "  \n",
    "    result.append(response['Body'].read().decode())\n",
    "\n",
    "  return result\n",
    "\n",
    "input_json = { \"data\":\n",
    "        [[0,2017,10,10,4.02,2,12095,146,29693.0,5814,1,0,11,19,0,4.02],\n",
    "         [0,2017,10,10,4.02,2,12095,146,29693.0,5814,1,0,11,19,0,4.02],\n",
    "         [0,2017,10,10,4.02,2,12095,146,29693.0,5814,1,0,11,19,0,4.02],\n",
    "         [0,2017,10,10,4.02,2,12095,146,29693.0,5814,1,0,11,19,0,4.02]]\n",
    "}\n",
    "\n",
    "result = lambda_handler(input_json, _)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
